# ğŸ§  BDH Language Model (Post-Transformer AI)

A **character-level language model** built using a **post-Transformer architecture** called **BDH (Beyond Deep Transformers)**.  
This project demonstrates how alternative architectures can model language **without standard self-attention blocks**, focusing on **sparse latent interactions**.

---

## ğŸš€ Project Overview

- Trains a neural language model on the **Tiny Shakespeare dataset**
- Uses a **custom BDH architecture** instead of a traditional Transformer
- Works at **byte / character level**
- Supports **training and interactive text generation**
- Designed to be **hackathon, research, and demo ready**

---

## ğŸ§© Architecture Highlights

- âŒ No standard Transformer blocks
- âœ… Sparse latent projections
- âœ… Rotary positional encoding (RoPE-style)
- âœ… Custom attention via latent space interaction
- âœ… Lightweight & research-oriented design

> This model explores ideas **beyond Transformers**, aligning with modern research directions in efficient and alternative sequence modeling.

---

## ğŸ“ Project Structure

