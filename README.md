#  BDH Language Model (Post-Transformer AI)

A **character-level language model** built using a **post-Transformer architecture** called **BDH (Beyond Deep Transformers)**.  
This project demonstrates how alternative architectures can model language **without standard self-attention blocks**, focusing on **sparse latent interactions**.

---

##  Project Overview

- Trains a neural language model on the **Tiny Shakespeare dataset**
- Uses a **custom BDH architecture** instead of a traditional Transformer
- Works at **byte / character level**
- Supports **training and interactive text generation**
- Designed to be **hackathon, research, and demo ready**

---

##  Architecture Highlights

- âŒ No standard Transformer blocks
- âœ… Sparse latent projections
- âœ… Rotary positional encoding (RoPE-style)
- âœ… Custom attention via latent space interaction
- âœ… Lightweight & research-oriented design

> This model explores ideas **beyond Transformers**, aligning with modern research directions in efficient and alternative sequence modeling.

---

##  Project Structure

## ğŸ“ Project Structure

```text
BDH/
â”œâ”€â”€ train.py          # Script for training the BDH language model
â”œâ”€â”€ infer.py          # Script for text generation (inference)
â”œâ”€â”€ bdh.py            # Core BDH model architecture
â”œâ”€â”€ input.txt         # Training dataset (Tiny Shakespeare)
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ README.md         # Project documentation
```
## Output 
<img width="1436" height="211" alt="image" src="https://github.com/user-attachments/assets/f50913bd-aa65-48e0-acd1-25b3fc589748" />

